\chapter{Background}

This section will explore background knowledge that pertains to the findings in
this thesis. In order to understand the significance of a sampling method, we
must first explore the fundamentals of Monte Carlo integration and how
different sampling strategies can affect the effectiveness of Monte Carlo
integration. The background section will discuss different conventional
sampling techniques for Monte Carlo integration and comment on various
properties of these sampling techniques. \\

\section{Monte Carlo Integration}

Suppose you have a square dartboard that is one meter by one meter. We will
call the length of a side $l$. We know that the area of a square is simply
$l^2$, so the area of the dartboard is $1$. Now suppose we have a perfect
circle inside of the square, and we don't know the area of this circle. For
some reason, we have an unlimited supply of darts, and we know whether each
dart we throw falls inside the circle, or outside of the circle (assuming that
every dart hits the dartboard). If we want to find the area of the circle, we
can throw a bunch of random darts, record the number of darts that hit the
circle, and use the ratio of the number of darts that hit the circle to the
total number of darts thrown to find the area of the circle. For example, if
about a quarter of our darts hit the circle, then we can reasonably estimate
that the size of our circle is $0.25 * 1 = 0.25$. \\

The principle of stochastically estimating some arbitrary area or shape is the
same principle that underlies Monte Carlo integration. Instead of a perfectly
square dartboard, imagine an arbitrary integration domain. Now replace the
circle with some arbitrary function. We can define the integral of the function
as the area under the function. Now imagine throwing darts at random, within
some domain, except now we are trying to find the integral of a function rather
than the area of a square. \\

\todo{add a visual here with the circle/dartboard example}

In practice, Monte Carlo integration involves randomly selecting some point in
the function's domain ($R^d$) a certain number of times ($n$) and averaging the
calculated values.

\todo{add a visual here with an actual function}

\begin{equation}
    \int_{a}^{b} f(x) \approx \frac{1}{N} \sum_{i = 1}^{i = N} f(\overline{x}_i)
\end{equation}

\noindent % don't indent since this is a continuation of the prev paragraph
where $\overline{x}$ is a set of uniform random numbers and $\overline{x}_i$ is
the $i^{th}$ element of the set of uniform random numbers. It is important to
note that is is not strictly necessary to use uniform random sampling for Monte
Carlo integration, but doing so makes the explanation of Monte Carlo simpler.
An important property of Monte Carlo integration is that the estimation is
guaranteed to converge towards the correct answer as $N$ approaches infinity.
As a result, $N$ becomes a controlled variable that allows one to choose
between speed and accuracy.

\begin{equation}
    \lim_{N \rightarrow \inf} \frac{1}{N} \sum_{i = 1}^{i = N}
    f(\overline{x}_i) = \int_{a}^{b} f(x)
\end{equation}

\subsection{Convergence Rate}

Even if you know that Monte Carlo integration will eventually converge to the
correct answer, it is useful to know how quickly it will converge. We call this
property the \textbf{convergence rate}. One thing that makes Monte Carlo
integration so appealing for many use cases is that its convergence rate is
independent of the dimensionality of the integrand. Whether your integrand is a
2D or a 90D integrand, the accuracy of the approximate integral will increase
at the same rate. Monte Carlo integration with uniform random sampling has an
asymptotic convergence rate of $O(N^{-1})$. While Monte Carlo has the advantage
of having a convergence rate that doesn not scale with dimensionality, there
are methods that make it possible to yield a steeper convergence rate.

\section{Sampling Methods}

One heavily researched method to speed up Monte Carlo integration entails
changing the sampling strategy. You can recall the example with the dartboard,
and you may realize that it may be more efficient to ensure that every part of
the dartboard as a dart in it. Instead of randomly throwing darts at the board,
what if you were to break the dartboard up into small grids. Doing so would
ensure that you were at least covering all parts of the dartboard, alleviating
the worry that you might have missed something on the board. \\

Many sampling strategies were designed to fix exactly the same issue. Naive
random sampling can fail by oversampling or undersampling critical parts of a
function that will heavily influence the average (usually parts of a function
that are really large or really small relative to the average value of the
function). In this section, we will explore notable sapmling methods that are
used in Monte Carlo integration that we considered when developing the new
sampling methods that will be introduced in this paper.

\subsection{Random Sampling}

We have already discussed one method for Monte Carlo sampling: uniform random
sampling. There are several advantages to using this method, namely that it is
relatively easy to implement correctly and is a fairly simple method. \\

There is a very intuitive pitfall that results from using naive random sampling
- consider a scenario where we are trying to sample a step function in which
$f(x) = 0$ for $ x \leq 0.5$ and $f(x) = 1$ for $ x > 0.5$. In this case, if it
so happens that every sample falls in the region where $f(x) = 0$, then the
estimated integral will be $0$, which is clearly incorrect. Random sampling
presents the danger of oversampling or undersampling regions of functions.
This phenomenon becomes even more dangerous with functions that are not very
consistent across the sampled domain.

\subsection{N-Rooks Sampling}

N-Rooks sampling is a method which yields a perfect 1D distribution of points.
Imagine a finite 1D line. Now suppose that we want to place $N$ points on the
line, ensuring that none of the points ``overlap.'' An easy method to do this is
to divide the line into $N$ sections, or \textbf{strata}, and place a point
within each strata. Because there are $N$ points and $N$ strata, we know that
there will only be one point per strata without any overlap. We can randomly
jitter a point within a strata as well to provide a nice ``random'' looking set
of points that also have a good numerical distribution. \\

N-Rooks sampling can be extended to multiple dimensions in an uncorrelated
manner by generating an N-Rooks point set for each dimension.

\subsection{Jittered Sampling}

In the jittered sampling approach, you divide the domain into different strata,
and place one sample randomly inside each strata.  Stratifying your sample
domain ensures that each stratum has a sample inside of it, which alleviates
some of the issues with uniform random sampling. For example, if we define even
just two strata in the case discussed for uniform random sampling, with the
step function, we can avoid the pitfall of only sampling the side where $f(x) =
0$. \\

This method has the advantage of being relatively simple to implement, and it
scales easily to multiple dimensions (you can jitter in 2D by creating a grid
of squares within the domain, jittering in 3D is the same except you select
cubes, etc). Jittered still does not provide the most efficient manner of
sampling a function, as it has the unintended effect of clumping points near
the boundaries of multiple strata.

\subsection{Multi-Jittered Sampling}

This sampling method was introduced by Pete Shirley and introduces a new method
of sampling that addresses the issue of functions that have variance in two
dimensions. Shirley wanted to address the potential clumping of points that
tends to happen with jittered sampling. His solution was to combine the
constraints of jittered sampling with the constraints of N-rooks. This yields a
perfect distribution in each 1D projection as well as a nice distribution in
2D. \\

The technique to generate a multi-jittered point set is to start out by first
dividing the domain (2D) into a set of strata and corresponding substrata.
Start by placing a point in each strata, forming the ``canonical'' arrangement.
Note that the canonical arrangement maintains the jittered and N-Rooks
constraints. Then shuffle the samples within each strata in each dimension,
while maintaining the jittered and N-Rooks constraints.

\subsection{Correlated Multi-Jittered Sampling}

Introduced by researcher Andrew Kensler of Pixar in 2013, correlated
multi-jittered (CMJ) sampling modifies Shirley's multi-jittered sampling
design to provide point sets that have a better shuffling arrangement and
consequently yield steeper convergence rates in empirical testing. \\

The key idea behind CMJ is changing the shuffling step from multi-jittered
sampling. In Shirley's method, the strata in each dimension shuffles points
independently of the shuffle at other strata. Kensler's modification entails
using the same shuffle over each strata in each dimension.

\chapter{Background}

This section will explore background knowledge that pertains to the findings in
this thesis. In order to understand the significance of a sampling method, we
must first explore the fundamentals of Monte Carlo integration and how
different sampling strategies can affect the effectiveness of Monte Carlo
integration. The background section will discuss different conventional
sampling techniques for Monte Carlo integration.

\section{Monte Carlo Integration}

Suppose we have a square dartboard that is one meter by one meter. We will call
the length of a side $l$. We know that the area of a square is simply $l^2$, so
the area of the dartboard is $1$. Now suppose we have a perfect circle inside of
the square, and we do not know the area of this circle. We have an unlimited
supply of darts, and we know whether each dart we throw falls inside the circle,
or outside of the circle (assuming that every dart hits the dartboard). If we
want to find the area of the circle, we can throw a bunch of random darts,
record the number of darts that hit the circle, and use the ratio of the number
of darts that hit the circle to the total number of darts thrown to find the
area of the circle. For example, if about a quarter of our darts hit the circle,
then we can reasonably estimate that the size of our circle is $0.25 \cdot 1 =
0.25$. \\

The principle of stochastically estimating some arbitrary area or shape is the
same principle that underlies Monte Carlo integration. Instead of a perfectly
square dartboard, imagine an arbitrary integration domain. Now replace the
circle with some arbitrary function. We can define the integral of the function
as the area under the function. Now imagine throwing darts at random, within
some domain, except now we are trying to find the integral of a function rather
than the area of a circle. \\

In practice, Monte Carlo integration involves randomly selecting some point in
the function's domain ($\mathbb{R}^d$) a certain number of times ($N$) and
averaging the calculated values.

\begin{equation}
    \int_{a}^{b} f(x) \approx \frac{1}{N} \sum_{i = 1}^{i = N} f(\overline{x}_i)
\end{equation}

\noindent % don't indent since this is a continuation of the prev paragraph
where $\overline{x}_i$ is a uniform random number. It is important to note that
it is not strictly necessary to use uniform random sampling for Monte Carlo
integration, but doing so makes the explanation of Monte Carlo simpler.  An
important property of Monte Carlo integration is that the estimation is
guaranteed to converge towards the correct answer as $N$ approaches infinity.
As a result, $N$ becomes a variable that allows one to choose between speed and
accuracy.

\begin{equation}
    \lim_{N \rightarrow \inf} \frac{1}{N} \sum_{i = 1}^{i = N}
    f(\overline{x}_i) = \int_{a}^{b} f(x)
\end{equation}

\subsection{Variance}

Generally, we want to know how reliable a numerical method is. If we have some
method of approximating a value, it seems useful to know whether the answer we
get will vary wildly across different runs, or whether it tends to be fairly
consistent. In most cases, we want more consistency, something that becomes
difficult to have with the element of randomness. The measure of how
inconsistent these approximations are is called \textbf{variance}, also denoted
as $\sigma^2$.

\subsection{Convergence Rate}

Even if we know that Monte Carlo integration will eventually converge to the
correct answer, it is useful to know how quickly it will converge. We call this
property the \textbf{convergence rate}. One thing that makes Monte Carlo
integration so appealing for many use cases is that its convergence rate is
independent of the dimensionality of the integrand. Whether our integrand is a
2D or a 90D integrand, the accuracy of the approximate integral will increase at
the same rate. While Monte Carlo with uncorrelated sampling has the advantage of
having a convergence rate that does not scale with dimensionality, there are
methods that make it possible to yield a steeper convergence rate. The
convergence rate of Monte Carlo integration with a particular sampling method
can be calculated theoretically given knowledge about the sampling methods and
integrand, which yields an asymptotic bound as with random sampling, or
empirically, by measuring the variance of a sampling method at various sample
counts with a high number of trials. As part of our research, we ascertained the
theoretical convergence bounds for various integrands and samplers, and verified
them empirically.

\begin{equation}
    \sigma^2 = E[(X - \mu^2)]
\end{equation}

\noindent
where $E$ denotes the \textbf{expected value}, $\mu$ is the average value of
many runs, and $X$ is simply a random variable \cite{Loeve:1977:Probability}. In
this case, $X$ is the result of some run of a Monte Carlo integration. \\

If we get the same value every single time, then the difference
between the result of a particular and the average of all the runs will be 0.
Clearly, if the answers are more consistent, they will vary less. Variance gives
us a way to quantify the consistency of random variables, or variables that are
the result of some sort of random process. It is important to note that variance
is not the same as accuracy - while having low variance is desirable, it does
not necessarily imply that the answers that we get are correct.

\section{Sampling}

One heavily researched method to speed up Monte Carlo integration entails
changing the sampling strategy. Recall the example with the dartboard. It may be
more efficient to ensure that every part of the dartboard has a dart in it.
What if instead of randomly throwing darts at the board, we were to break the
dartboard up into small grids? Doing so would ensure that we were at least
covering all parts of the dartboard, alleviating the worry that we might have
missed something on the board. \\

Many sampling strategies were designed to fix exactly this issue. Naive random
sampling can fail by oversampling or undersampling critical parts of a function
that will heavily influence the average (usually parts of a function that are
really large or really small relative to the average value of the function). In
this section, we will explore notable sampling methods that are used in Monte
Carlo integration that we considered when developing the new sampling methods
that will be introduced in this paper. \\

While this section will describe and explore many sampling methods, this is not
necessarily a comprehensive survey of all Monte Carlo sampling methods. These
are important sampling methods that we looked at as prior work as we worked on
orthogonal array based sampling.

\subsection{Discrepancy}

Some common properties of sampling methods that are often used to describe a
point set include something called the \textbf{discrepancy} of a point set. The
discrepancy is a numerical measure of how spaced out a point set is
\cite{Shirley:1991:Discrepancy}. We do not want our points to be clumped
together, and ensuring that all the points are well spaced out tends to be a
property that is very important for Monte Carlo sampling. In the literature,
many points sets are referred to in a favorable manner by being described as a
low-discrepancy sequence. There are several ways to measure discrepancy, but in
this thesis, we do not actually calculate the discrepancy values of different
point sets, so it is not important to know the mathematical definition so much
as the general implication of the term. \\

\subsection{Padding}

There is a common technique used specifically for high-dimensional integrands
called \textbf{padding} which combines different instances or shuffles of a
low-dimensional sequence or point set to create a higher dimensional point set.
Suppose that we have some point set that has a favorable 2D distribution. We
want to use this point set, which is well stratified in two dimensions, on a
four dimensional integrand. Suppose that we have $N$ samples, which consist of
two-tuples (the first element representing the first dimension, and the second
element representing the value in the second dimension). Now shuffle the indices
of the point set, and retain that list of numbers, calling it $L$, so that the
first element of the original point set corresponds to the first element in $L$.
We can use these shuffled indices to add two more dimensions to the point set,
by selecting some index, $i$, from the original point set, and ``padding'' it
with some other index $l$ which is imply the $i^{th}$ element of $L$, yielding
four dimensions. This method is called uncorrelated jittering
\cite{Cook:1984:Distributed}. There have been other methods proposed for
padding, such as Owen Scrambling \cite{Owen:1997:Monte} and random digit XOR
scrambling \cite{Kollig:2002:Efficient}. \\

Padding provides an obvious advantage with respect to efficiency. Consider that
we can effectively ``re-use'' a point set to add more dimensions to it without
actually computing or storing more samples. If we only need a good distribution
in a lower set of dimensions than the dimensionality of the integrand, then
padding is an easy way to add the dimensions we need.

\subsection{Random Sampling}

\begin{figure}
    \centering
    \includegraphics[scale=0.15]{figures/random_sampler.eps}
    \caption{Random sampler, 256 points}
    \label{random-sampler-example}
\end{figure}

We have already discussed one method for Monte Carlo sampling: uniform random
sampling. There are several advantages to using this method, namely that it is
relatively easy to implement correctly and simple to understand. We can see what
such a point set looks like in Figure~\ref{random-sampler-example}. \\

There is an intuitive pitfall that results from using naive random sampling.
Consider a scenario where we are trying to sample a step function in which $f(x)
= 0$ for $ x \leq 0.5$ and $f(x) = 1$ for $ x > 0.5$. In this case, if it so
happens that every sample falls in the region where $f(x) = 0$, then the
estimated integral will be $0$, which is clearly incorrect. Random sampling
presents the danger of oversampling or undersampling regions of functions. This
phenomenon becomes even more dangerous with functions that are vary more across
the sampled domain.

\subsection{N-Rooks Sampling}

\begin{figure}
    \centering
    \includegraphics[scale=0.15]{figures/n_rooks_sampler.eps}
    \caption{N-rooks sampler, 256 points}
    \label{n-rooks-sampler-example}
\end{figure}

N-Rooks sampling is a method which yields a perfect 1D distribution of points.
Imagine a finite 1D line. Now suppose that we want to place $N$ points on the
line, ensuring that none of the points ``overlap.'' An easy method to do this is
to divide the line into $N$ sections, or \textbf{strata}, and place a point
within each stratum. We can randomly additionally jitter a point within a strata
to provide a ``random'' looking set of points while maintaining stratification.
Refer to Figure~\ref{n-rooks-sampler-example} to see what this distribution
looks like in two dimensions. \\

N-Rooks sampling can be extended to multiple dimensions by generating an N-Rooks
point set for each dimension and arbitrarily padding them together. This method
tends to be suboptimal but has the advantage of being trivial to implement and
not very computationally intensive. As we can see in
Figure~\ref{n-rooks-sampler-example}, the points are well-stratified in 1D
projections, but is not stratified in two dimensions. These properties lead to
lower discrepancy and are not desirable for Monte Carlo integration.

\subsection{Jittered Sampling}

\begin{figure}
    \centering
    \includegraphics[scale=0.15]{figures/jittered_sampler.eps}
    \caption{Jittered sampler, 256 points}
    \label{jittered-sampler-example}
\end{figure}

In the jittered sampling approach, we divide the domain into different strata
and place one sample randomly inside each stratum. We can see what this looks
like by referring to Figure~\ref{jittered-sampler-example}. Stratifying our
sample domain ensures that each stratum has a sample inside of it, which
alleviates some of the issues with uniform random sampling. For example, if we
define even just two strata in the case discussed for uniform random sampling,
with the step function, we can avoid the pitfall of only sampling the side where
$f(x) = 0$. \\

This method has the advantage of being relatively simple to implement, and it
can be extended in order to work in multiple dimensions. we can jitter in 2D by
creating a grid of squares within the domain, jittering in 3D is the same except
we select cubes, etc. Jittering falls under the ``curse of dimensionality,'' so
it does not scale to higher dimensions efficiently, and scaling to higher and
higher dimensions is progressively more expensive. Jittered sampling does not
provide the most effective manner of sampling a function, as it has the
unintended effect of clumping points near the boundaries of strata.

\subsection{Multi-Jittered Sampling}

\begin{figure}
    \centering
    \includegraphics[scale=0.15]{figures/mj_canonical.eps}
    \caption{The ``canonical'' multi-jittered arrangement, 256 points}
    \label{mj-canonical-sampler}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.15]{figures/mj_randomized.eps}
    \caption{Randomized multi-jittered sampler, 256 points}
    \label{mj-randomized-sampler}
\end{figure}

This sampling method was introduced by Pete Shirley to address functions that
have variance in two dimensions \cite{Shirley:1991:Discrepancy}. Shirley wanted
to address the potential clumping of points that tends to happen with jittered
sampling. His solution was to combine the constraints of jittered sampling with
the constraints of N-rooks.  This yields a perfect distribution in each 1D
projection as well as a nice distribution in 2D. \\

The technique to generate a multi-jittered point set is to start out by first
dividing the domain (2D) into a set of strata and corresponding substrata.
Start by placing a point in each stratum, forming the ``canonical'' arrangement,
which we can see in Figure~\ref{mj-canonical-sampler}. Note that the canonical
arrangement maintains the jittered and N-Rooks constraints. Then shuffle the
samples within each stratum in each dimension, while maintaining the jittered
and N-Rooks constraints, which we can see in Figure~\ref{mj-randomized-sampler}.

% TODO Refer to comments from Wojciech, elaborate on the technique used to
% generate MJ

\subsection{Correlated Multi-Jittered Sampling}

\begin{figure}
    \centering
    \includegraphics[scale=0.15]{figures/cmj_sampler.eps}
    \caption{Correlated multi-jittered sampler, 225 points}
    \label{cmj-sampler-example}
\end{figure}

Introduced by Andrew Kensler, correlated multi-jittered (CMJ) sampling modifies
Shirley's multi-jittered sampling design to provide point sets that have a
better shuffling arrangement and consequently yield steeper convergence rates in
empirical testing \cite{Kensler:2013:Correlated}. \\

The key idea behind CMJ is changing the shuffling step from multi-jittered
sampling. In Shirley's method, the strata in each dimension shuffle points
independently of the shuffle at other strata. Kensler's modification entails
using the same shuffle over each stratum in each dimension
\cite{Kensler:2013:Correlated}. Qualitatively, these points also look more
aesthetically pleasing given the stronger constraints that CMJ has for its
points as we can see in Figure~\ref{cmj-sampler-example}.

\subsection{Quasi Monte Carlo Sampling}

All of the sampling methods discussed so far rely on some element of randomness.
Each produced point has some sort of input that stems from pseudo-random number
generation. There are methods of sampling that have been developed that do not
rely on any sort of randomness. We call such sampling methods ``Quasi-Monte
Carlo'' (QMC) sampling. \\

QMC sampling methods tend to provide much higher performance in Monte Carlo
integration than other sampling methods and have become quite popular due to
their superior performance, low discrepancy, and high efficiency. \\

There are some common point sets and sequences in QMC that are commonly referred
to in the literature, such as \textbf{(t, s) sequences} and \textbf{(t, m, s)
    nets}. Such terms describe stratification guarantees for samplers in a
convenient and consistent manner. To explain these terms, we must first define
the \textbf{elementary interval}, which we describe as the elementary
$s$-interval in base $b$. Suppose we have some interval with the following
properties:

\begin{equation}
    \prod^{s}_{j = 1} \bigg[ \frac{a_j}{b^{d_j}}, \frac{a_j + 1}{b^{d_j}}
    \bigg]
\end{equation}

Note that the invariants $s \geq 1$ and $b \geq 2$ must hold for this definition
\cite{NIEDERREITER198851}. We can see that the elementary interval provides a
notation that describes some sort of even partitioning within an arbitrary
domain. Having this domain, consisting of evenly spaced partitions, naturally
falls within the scope of sampling. This talk of elementary intervals is very
similar to the idea of stratification, but intervals provide a far more formal
and explicit bound. \\

Now let us define a $(t, m, s)$-net in base $b$. We can define one as a set of
$b^m$ points in a domain $[0, 1)^s$ such that if we define some sequence $x_i$,
the cardinality of some elementary interval $P \cup \{ x_1, \cdots, x_{b^m} \} =
b^t$ for every elementary interval $P$ in base $b$ such that the hypervolume of
$P = b^{t - m}$ \cite{NIEDERREITER198851}. While the formal mathematical
definition seems daunting, the more general implication of the $(t, m, s)$-net
constraints is that some set of points is distributed in a uniform manner which
is more acutely described by the properties of the net, $t, m, s$. \\

A $(t, s)$-sequence is somewhat based on $(t, m, s)$-nets. A $(t, s)$-net is an
infinite sequence which requires that for every $m > t$ and every $k \geq 0$,
the set $N$ is defined by Equation~\eqref{eq:ts-sequence}
\cite{NIEDERREITER198851}, where $N$ must be a valid $(t, m, s)$ net in base
$b$.

\begin{equation}
    N = \{[x_i] : kb^m \leq i < (k + 1)b^n \}\label{eq:ts-sequence}
\end{equation}

\subsection{Halton Sequence}

\begin{figure}
    \centering
    \includegraphics[scale=0.15]{figures/halton_sampler.eps}
    \caption[Halton sampler]{Halton sampler, 256 points}
    \label{halton-sampler-example}
\end{figure}

The Halton sequence, named after its author, was introduced in 1960 as a means
of deterministically generating points that have low discrepancy, but at the
same time, appear to look somewhat random \cite{Halton:1964:ARQ:355588.365104}
(refer to Figure~\ref{halton-sampler-example}). The Halton sequence has the
additional advantage of being a progressive sequence - we do not need knowledge
of the previous points in order to generate the next point. This property
allows for extremely high efficiency in generating samples. \\

The Halton sequence can be constructed as follows: given some index in the point
set, get the representation of the number in binary. Take the number, invert it
by reading the number from right to left, and then place that number after a
decimal point. Interpreting that number in base 10 yields the point in the
$[0,1)^d$ domain. \\

As an example, let us look at the point at the fifth index of the Halton
sequence.  $5 = 101$. $101$ inverted is $101$. Placing the number after a
decimal point yields $0.101$, which we interpret in base 10. It is easy to
generate points in this manner, as we can trivially convert an index into binary
and perform these steps to calculate where the point lies in space with high
computational efficiency and very low computational overhead.

\subsection{Sobol Sequence}

\begin{figure}
    \centering
    \includegraphics[scale=0.15]{figures/sobol_sampler.eps}
    \caption[Sobol sampler]{Sobol sampler, 256 points}
    \label{sobol-sampler-example}
\end{figure}

Perhaps one of the most widely used sequences in Monte Carlo rendering and other
Monte Carlo applications, the Sobol sequence provides a high-efficiency,
low-discrepancy point set that performs extremely well in Monte Carlo
integration. We can think of Sobol as the current gold standard of Monte Carlo
sampling. Not only does Sobol perform well, it is also a progressive sequence,
so we do not need to know the number of samples we will use beforehand, for
example, which is a limitation present in many other sampling methods. The Sobol
sequence exploits the properties of numbers in their binary representations in
order to progressively refine the intervals that points are distributed across
\cite{SOBOL196786} (refer to Figure~\ref{sobol-sampler-example}). \\

The Sobol sequence has the property of being well-distributed in the unit
domain and can progressively partition the domain as more samples are generated
to maintain a good distribution. The progressive nature of Sobol sequencing
makes it very attractive to use in any scenario where we would want to have
incremental sampling, such as computer graphics.

\section{Monte Carlo in Computer Graphics}

While we have extensively discussed the usage of Monte Carlo sampling and how it
pertains to integration in general, this does not explain the relevance of Monte
Carlo sampling methods in the context of computer graphics, and specifically
pertaining to rendering. \\

We can generally think of the value of a pixel in a rendered image as the result
of the integral of light multiplied by the color value. In 1986, James Kajiya
introduced the rendering equation \eqref{eq:rendering}, perhaps one of the most
well-known and fundamental equations in graphics
\cite{Kajiya:1986:RE:15886.15902}.

\begin{equation}
    L_o(x, \omega_o,\lambda, t) = L_e(x, \omega_o, \lambda, t) + \int_{\Omega}
    f_r(x, \omega_i, \omega_o, \lambda, t) L_i(x, \omega_i, \lambda, t)
    (\omega_i \cdot n) d \omega_i \label{eq:rendering}
\end{equation}

Integration is a necessary operation when calculating the radiance value of some
point. Monte Carlo integration offers an way to approximate this equation, which
is extremely difficult to calculate analytically. Not only that, the rendering
equation tends to be extremely high dimensional, and it becomes even more
practical to use Monte Carlo integration due to the fact that its convergence
rate is independent of dimensionality. Generally in rendering, taking a sample
is equivalent to tracing a ray, which is a computationally expensive operation.
\\

The practical benefit to speeding up Monte Carlo integration equates to time and
money saved when actually rendering a picture or a movie. It also equates to
increased accuracy if we want to use the same amount of time when rendering. In
either case, optimizing Monte Carlo integration carries obvious and significant
benefits for rendering.

\section{High-Dimensional Integration}

Many of the samplers we presented focus on stratifying and addressing variance
in two dimensions. Rendering is extremely high dimensional and potentially
infinitely dimensional. The functions that Monte Carlo rendering attempts to
approximate often exhibit variance in more than two dimensions. Having sampling
methods that explicitly address this high-dimensional variance would offer a
significant improvement in efficiency. \\

One of the techniques explained earlier, padding, attempts to rectify this to
some degree. It generates high dimensional samples, but these samples are not
stratified in high dimensions; they are only well-stratified in pairs of
dimensions (if we have a base sampler that is two dimensional and padded
together). \\

Another issue that arises is that samplers that are well-stratified in higher
dimensions still exhibit poor stratification in certain projections. Consider a
four-dimensional sampler, with the dimensions $\{0, 1, 2, 3\}$. A sampler that
is well-stratified in four dimensions and two dimensions can still suffer if we
take some arbitrary cross-dimensional projections, such as dimension $0$ and
dimension $3$. \\

The fact that arbitrary projections can exhibit poor stratification, or even the
fact that many samplers do not exhibit stratification in high dimensions, can
cause serious issues with Monte Carlo integration. If there is a mismatch
between stratification and the dimensions in which a function exhibits variance,
the performance of Monte Carlo will degrade to $O(N^{-1})$
\cite{Singh:2017:Convergence}.
